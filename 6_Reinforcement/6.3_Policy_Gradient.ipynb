{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve cartpole with REINFORCE\n",
    "\n",
    "> By Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk))\n",
    "\n",
    "In this part, we will create an agent that can learn to solve the [cartpole problem](https://gym.openai.com/envs/CartPole-v0/) from OpenAI Gym by applying a simple policy gradient method called REINFORCE.\n",
    "In the cartpole problem, we need to balance a pole on a cart that moves along a track by applying left and right forces to the cart.\n",
    "\n",
    "We will implement a probabilistic policy, that given a state of the environment, $s$, outputs a probability distribution over available actions, $a$:\n",
    "\n",
    "$$\n",
    "p_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "The policy is a neural network with parameters $\\theta$ that can be trained with gradient descent.\n",
    "When the set of available actions is discrete, we use a network with softmax output.\n",
    "The core idea of training the policy network is quite simple: *we want to maximize the expected total reward by increasing the probability of good actions and decreasing the probability of bad actions*. \n",
    "\n",
    "To achieve this, we apply the gradient of the expected discounted total reward (return):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] &= \\nabla_\\theta \\int p_\\theta(a|s) R({a}) \\, da \\\\\n",
    "&= \\int \\nabla_\\theta p_\\theta(a|s) R(a)  \\, da \\\\\n",
    "&= \\int p_\\theta(a|s) \\nabla_\\theta \\log p_\\theta(a|s) R(a) \\, da \\\\\n",
    "&= \\mathbb{E}[\\nabla_\\theta \\log p_\\theta(a|s) R(a)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "by definition of expectation and using the identity \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta p_\\theta(a|s) = p_\\theta(a|s) \\nabla_\\theta \\log p_\\theta(a|s) \\ .\n",
    "$$\n",
    "\n",
    "The expectation cannot be evaluated analytically, but we have an environment simulator that when supplied with our current policy $p_\\theta(a|s)$ can return a sequence of *actions*, *states* and *rewards*. This allows us to replace the integral with a Monte Carlo average\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{T} \\sum_{t=0}^T \\nabla_\\theta \\log p_\\theta(a_t|s_t) R_t \\ ,\n",
    "$$\n",
    "\n",
    "which is our final gradient estimator, also known as REINFORCE. In the Monte Carlo estimator we run the environment simulator for a predefined number of steps with actions chosen stochastically according to the current stochastic action network $p_\\theta(a|s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: For simple reinforcement learning problems (like the one we will address in this exercise) there are simpler methods that work just fine. However, the Policy Gradient method has been shown to also work well for complex problems with high dimensional inputs and many parameters, where simple methods become inadequate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from viewer import Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gym environment\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A state in this environment is four numbers describing the position of the cart along with the angle and speed of the pole.\n",
    "There are two available actions: push the cart *left* or *right* encoded as 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "print('sample state:', s)\n",
    "print('sample action:', a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the environment looks when we just take random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo the environment\n",
    "env.reset() # reset the environment\n",
    "view = Viewer(env, custom_render=True) # we use this custom viewer to render the environment inline in the notebook\n",
    "for _ in range(200):\n",
    "    view.render() # display the environment inline in the notebook\n",
    "    # env.render() # uncomment this to use gym's own render function\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "view.render(close=True, display_gif=True) # display the environment inline in the notebook\n",
    "# env.render(close=True) # uncomment this to use gym'm own render function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking random actions does not do a very good job at balancing the pole. Let us now apply the Policy Gradient method described above to solve this task!\n",
    "\n",
    "Let's first define our network and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"Policy network\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def loss(self, action_probabilities, returns):\n",
    "        return -torch.mean(torch.mul(torch.log(action_probabilities), returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, discount_factor):\n",
    "    \"\"\"Compute discounted returns.\"\"\"\n",
    "    returns = np.zeros(len(rewards))\n",
    "    returns[-1] = rewards[-1]\n",
    "    for t in reversed(range(len(rewards)-1)):\n",
    "        returns[t] = rewards[t] + discount_factor * returns[t+1]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, our policy will be a rather simple neural network with one hidden layer. We can retrieve the shape of the state space (input) and action space (output) from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = env.observation_space.shape[0]\n",
    "n_hidden = 20\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print('state shape:', n_inputs)\n",
    "print('action shape:', n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training settings\n",
    "\n",
    "num_episodes = 2000\n",
    "rollout_limit = env.spec.timestep_limit # max rollout length\n",
    "discount_factor = 1.0 # reward discount factor (gamma), 1.0 = no discount\n",
    "learning_rate = 0.001 # you know this by now\n",
    "val_freq = 100 # validation frequency\n",
    "\n",
    "# setup policy network\n",
    "\n",
    "policy = PolicyNet(n_inputs, n_hidden, n_outputs, learning_rate)\n",
    "\n",
    "# train policy network\n",
    "\n",
    "try:\n",
    "    training_rewards, losses = [], []\n",
    "    print('start training')\n",
    "    for i in range(num_episodes):\n",
    "        rollout = []\n",
    "        s = env.reset()\n",
    "        for j in range(rollout_limit):\n",
    "            # generate rollout by iteratively evaluating the current policy on the environment\n",
    "            with torch.no_grad():\n",
    "                a_prob = policy(torch.from_numpy(np.atleast_2d(s)).float())\n",
    "            a = (np.cumsum(a_prob.numpy()) > np.random.rand()).argmax() # sample action\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            rollout.append((s, a, r))\n",
    "            s = s1\n",
    "            if done: break\n",
    "        # prepare batch\n",
    "        rollout = np.array(rollout)\n",
    "        states = np.vstack(rollout[:,0])\n",
    "        actions = np.vstack(rollout[:,1])\n",
    "        rewards = np.array(rollout[:,2], dtype=float)\n",
    "        returns = compute_returns(rewards, discount_factor)\n",
    "        # policy gradient update\n",
    "        policy.optimizer.zero_grad()\n",
    "        a_probs = policy(torch.from_numpy(states).float()).gather(1, torch.from_numpy(actions)).view(-1)\n",
    "        loss = policy.loss(a_probs, torch.from_numpy(returns).float())\n",
    "        loss.backward()\n",
    "        policy.optimizer.step()\n",
    "        # bookkeeping\n",
    "        training_rewards.append(sum(rewards))\n",
    "        losses.append(loss.item())\n",
    "        # print\n",
    "        if (i+1) % val_freq == 0:\n",
    "            # validation\n",
    "            validation_rewards = []\n",
    "            for _ in range(10):\n",
    "                s = env.reset()\n",
    "                reward = 0\n",
    "                for _ in range(rollout_limit):\n",
    "                    with torch.no_grad():\n",
    "                        a = policy(torch.from_numpy(np.atleast_2d(s)).float()).argmax().item()\n",
    "                    s, r, done, _ = env.step(a)\n",
    "                    reward += r\n",
    "                    if done: break\n",
    "                validation_rewards.append(reward)\n",
    "            print('{:4d}. mean training reward: {:6.2f}, mean validation reward: {:6.2f}, mean loss: {:7.4f}'.format(i+1, np.mean(training_rewards[-val_freq:]), np.mean(validation_rewards), np.mean(losses[-val_freq:])))\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(211)\n",
    "plt.plot(range(1, len(training_rewards)+1), training_rewards, label='training reward')\n",
    "plt.plot(moving_average(training_rewards))\n",
    "plt.xlabel('episode'); plt.ylabel('reward')\n",
    "plt.xlim((0, len(training_rewards)))\n",
    "plt.legend(loc=4); plt.grid()\n",
    "plt.subplot(212)\n",
    "plt.plot(range(1, len(losses)+1), losses, label='loss')\n",
    "plt.plot(moving_average(losses))\n",
    "plt.xlabel('episode'); plt.ylabel('loss')\n",
    "plt.xlim((0, len(losses)))\n",
    "plt.legend(loc=4); plt.grid()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review solution\n",
    "s = env.reset()\n",
    "view = Viewer(env, custom_render=True)\n",
    "for _ in range(500):\n",
    "    view.render()\n",
    "    a = policy(torch.from_numpy(np.atleast_2d(s)).float()).argmax().item()\n",
    "    s, r, done, _ = env.step(a)\n",
    "view.render(close=True, display_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now it is your turn! Play around the code above and try to make it learn better and faster.\n",
    "\n",
    "Experiment with the:\n",
    "\n",
    "* number of episodes\n",
    "* discount factor\n",
    "* learning rate\n",
    "* number of hidden layers and units\n",
    "\n",
    "\n",
    "### Exercise 1 \n",
    "\n",
    "*Describe any changes you made to the code and why you think they improve the agent. Are you able to get solutions consistently?*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 2 \n",
    "\n",
    "*Consider the following sequence of rewards produced by an agent interacting with an environment for 10 timesteps:*\n",
    "\n",
    "[0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "\n",
    "* *What is the total reward?*\n",
    "* *What is the total future reward in each timestep?*\n",
    "* *What is the discounted future reward in each timestep if $\\gamma = 0.9$?*\n",
    "\n",
    "*[Hint: See introdution notebook.]*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "*You will sometimes observe the validation reward starts out lower than the training reward but as training progresses they cross. How can you explain this behavior?*\n",
    "\n",
    "*[Hint: Do we use the policy network in the same way during training and validation?]*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "*How does the policy gradient method we have used address the exploration-exploitation dilemma (see the introduction notebook for definition)?*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
