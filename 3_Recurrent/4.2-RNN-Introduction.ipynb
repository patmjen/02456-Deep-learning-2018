{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "This is heavily influenced by [hedgehoglabs/xor](https://github.com/hedgehoglabs/xor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNN)\n",
    "\n",
    "This notebook introduces the concept of Recurrent Neural Networks (RNN).\n",
    "You won't have as many programming tasks in this exercise, but do make sure that you read everyhing carefully, and understand what is going on.\n",
    "\n",
    "___\n",
    "\n",
    "A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.\n",
    "\n",
    "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
    "The idea is that the network should be able to use the previous computations as some form of memory and apply this to the future computation.\n",
    "An image may best explain how this is to be understood,\n",
    "\n",
    "![rnn-unroll image](../static_files/rnn-unfold.png)\n",
    "\n",
    "\n",
    "where it the network contains the following elements:\n",
    "\n",
    "- $x$ is the input sequence of samples, \n",
    "- $U$ is a weight matrix applied to the given input sample,\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
    "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
    "- $o$ is the resulting output.\n",
    "\n",
    "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
    "We have the following computations through the network:\n",
    "\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\n",
    "- $o_t = \\mathrm{softmax}(W\\,{h_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem of Parity\n",
    "(parity simply means wheather the number is even or odd)\n",
    "\n",
    "The exercise comes from the OpenAI [Requests for Research 2.0](https://blog.openai.com/requests-for-research-2/) warmup exercise.\n",
    "The exercise reads:\n",
    "\n",
    "> Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "> 1. Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get?\n",
    "> 1. Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n",
    "\n",
    "LSTM stands for 'Long short-term memory', and is an extension of the standard RNN model described above.\n",
    "An LSTM has been extended with a memory module that enables it to remember over longer sequences.\n",
    "\n",
    "In this exercise we will however **stick with a standard RNN instead of an LSTM**.\n",
    "(The LSTM model will be covered later. See [this excelent blogpost](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah for more on LSTM's.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the Data\n",
    "\n",
    "Before we do much of anything we need some data to work with.\n",
    "The code below we generate the binary vectors that we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils import rnn as rnn_utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "max_bits = 50\n",
    "batch_size = 8\n",
    "\n",
    "DEFAULT_NUM_BITS = 50\n",
    "DEFAULT_NUM_SEQUENCES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to load the data set\n",
    "\n",
    "class XORDataset(data.Dataset):\n",
    "    def __init__(self, num_sequences=DEFAULT_NUM_SEQUENCES, num_bits=DEFAULT_NUM_BITS):\n",
    "        self.num_sequences = num_sequences\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        self.features, self.labels = get_random_bits_parity(num_sequences, num_bits)\n",
    "\n",
    "        # expand the dimensions for the rnn\n",
    "        # [batch, bits] -> [batch, bits, 1]\n",
    "        self.features = np.expand_dims(self.features, -1)\n",
    "\n",
    "        # [batch, parity] -> [batch, parity, 1]\n",
    "        self.labels = np.expand_dims(self.labels, -1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index, :], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "def get_random_bits_parity(num_sequences, num_bits):\n",
    "    \"\"\"Generate random bit sequences and their parity. (Our features and labels).\n",
    "        Returns:\n",
    "            bit_sequences: A numpy array of bit sequences with shape [num_sequences, num_bits].\n",
    "            parity: A numpy array of even parity values corresponding to each bit\n",
    "                with shape [num_sequences, num_bits].\n",
    "        \"\"\"\n",
    "    bit_sequences = np.random.randint(2, size=(num_sequences, num_bits))\n",
    "\n",
    "    # if total number of ones is odd, set even parity bit to 1, otherwise 0\n",
    "    # https://en.wikipedia.org/wiki/Parity_bit\n",
    "\n",
    "#     bitsum = np.sum(bit_sequences, axis=1)  # use only the final result.\n",
    "    bitsum = np.cumsum(bit_sequences, axis=1)  # Teacher forcing\n",
    "\n",
    "    # if bitsum is even: False, odd: True\n",
    "    parity = bitsum % 2 != 0\n",
    "\n",
    "    return bit_sequences.astype('float32'), parity.astype('float32')\n",
    "\n",
    "train_loader = DataLoader(XORDataset(num_bits=max_bits), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it looks like.\n",
    "In order to make the print statement look nicer we print the transpose of the data - i.e. the data is `[seq_len, num_seq]` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "[[[1. 0. 1. 1. 0. 1. 1. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "  [1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "  [1. 0. 1. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 0. 1. 1. 1.]\n",
      "  [1. 1. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 1. 0. 1. 0. 0.]\n",
      "  [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "  [0. 1. 0. 0. 1. 1. 0. 1.]\n",
      "  [1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "  [1. 1. 0. 1. 0. 1. 0. 1.]\n",
      "  [1. 1. 0. 1. 0. 0. 1. 1.]\n",
      "  [0. 0. 1. 0. 1. 1. 1. 0.]\n",
      "  [1. 1. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 1. 1. 0. 0.]]]\n",
      "[[30. 23. 24. 26. 23. 32. 26. 26.]]\n",
      "\n",
      "targets\n",
      "[[[1. 0. 1. 1. 0. 1. 1. 0.]\n",
      "  [1. 0. 0. 1. 0. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 0. 0. 1. 0.]\n",
      "  [1. 0. 0. 1. 1. 0. 1. 0.]\n",
      "  [1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 1. 0. 1.]\n",
      "  [0. 1. 1. 1. 0. 0. 0. 1.]\n",
      "  [1. 0. 0. 1. 0. 1. 1. 0.]\n",
      "  [1. 1. 0. 1. 1. 0. 1. 1.]\n",
      "  [0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "  [1. 1. 0. 1. 1. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 1. 1. 1.]\n",
      "  [1. 1. 1. 0. 0. 1. 1. 0.]\n",
      "  [1. 1. 1. 0. 1. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets  in train_loader:\n",
    "    print('inputs')\n",
    "    print(inputs.numpy()[:,:15].T)\n",
    "    print(np.sum(inputs.numpy(), 1).T)\n",
    "    print()\n",
    "    print('targets')\n",
    "    print(targets.numpy()[:,:15].T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than only have a target for the very last element in the sequence, we make a target for every sub-sequence along the way.\n",
    "During training we use all these targets to help the network learn along the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Model\n",
    "\n",
    "One of the nice things with using a library like `PyTorch` is that a lot of the complexity is hidden away.\n",
    "Creating a RNN is therefore not much different than creating any other kind of network, as we shall see below.\n",
    "Do however make sure to understand the concepts - they will help you debug, or when you want to work with more advanced concepts.\n",
    "\n",
    "Things to note however:\n",
    " * `batch_first=True` determines the order of the sequence. Normally in `PyTorch` the input is expected to be `[seq_len, batch, input_size]`, but this is sometimes inconvenient, so we use `batch_first=True` to make the input `[batch, seq, feature]`. \n",
    " * `pack_padded_sequence` and `pad_packed_sequence` are used for variable length sequences when the sequences don't have the same length. (Computers don't deal well with sequences of varing length, so we use padding to make them align).\n",
    " \n",
    "For more on RNNs in `PyTorch` [see here](https://pytorch.org/docs/stable/nn.html#torch.nn.RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = torch.nn.RNN(\n",
    "                batch_first=True,\n",
    "                input_size=1,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=2)\n",
    "\n",
    "        self.hidden_to_logits = torch.nn.Linear(hidden_size, 1)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # pack the inputs\n",
    "        packed_inputs = rnn_utils.pack_padded_sequence(\n",
    "                inputs, lengths, batch_first=True).to(device)\n",
    "\n",
    "        rnn_out, _ = self.rnn(packed_inputs)\n",
    "\n",
    "        unpacked, _ = rnn_utils.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "\n",
    "        logits = self.hidden_to_logits(unpacked)\n",
    "        predictions = self.activation(logits)\n",
    "\n",
    "        return logits, predictions\n",
    "\n",
    "hidden_size = 4\n",
    "\n",
    "model = RNN(hidden_size, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As allways we need to define our optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "lr = 1\n",
    "momentum = 0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions - you don't need to worry about them.\n",
    "\n",
    "def adjust_lengths(vary_lengths, inputs, targets):\n",
    "    batch_size = inputs.size()[0]\n",
    "    max_bits = inputs.size()[1]\n",
    "\n",
    "    if not vary_lengths:\n",
    "        lengths = torch.ones(batch_size, dtype=torch.int) * max_bits\n",
    "        return lengths\n",
    "\n",
    "    # choose random lengths\n",
    "    lengths = np.random.randint(1, max_bits, size=batch_size, dtype=int)\n",
    "\n",
    "    # keep one the max size so we don't need to resize targets for the loss\n",
    "    lengths[0] = max_bits\n",
    "\n",
    "    # sort in descending order\n",
    "    lengths = -np.sort(-lengths)\n",
    "\n",
    "    # chop the bits based on lengths\n",
    "    for i, sample_length in enumerate(lengths):\n",
    "        inputs[i, lengths[i]:, ] = 0\n",
    "        targets[i, lengths[i]:, ] = 0\n",
    "\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def evaluate(max_bits, vary_lengths, device, model):\n",
    "    # evaluate on more bits than training to ensure generalization\n",
    "    valid_loader = DataLoader(\n",
    "            XORDataset(num_sequences=5000, num_bits=int(max_bits * 1.5)), batch_size=500)\n",
    "\n",
    "    is_correct = np.array([])\n",
    "\n",
    "    for inputs, targets in valid_loader:\n",
    "        lengths = adjust_lengths(vary_lengths, inputs, targets)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, predictions = model(inputs, lengths)\n",
    "            is_correct = np.append(is_correct, ((predictions > 0.5) == (targets > 0.5)))\n",
    "\n",
    "    accuracy = is_correct.mean()\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "We are now ready to train the network. \n",
    "This is also very similar to what we have already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, step   250. Train summary: loss 0.3283, accuracy 0.760\n",
      "epoch  1, step   500. Train summary: loss 0.3739, accuracy 0.702\n",
      "epoch  1, step   750. Train summary: loss 0.4137, accuracy 0.697\n",
      "epoch  1, step  1000. Train summary: loss 0.3441, accuracy 0.772\n",
      "Validation accuracy 0.771\n",
      "\n",
      "epoch  1, step  1250. Train summary: loss 0.5089, accuracy 0.658\n",
      "epoch  1, step  1500. Train summary: loss 0.2600, accuracy 0.808\n",
      "epoch  1, step  1750. Train summary: loss 0.3501, accuracy 0.738\n",
      "epoch  1, step  2000. Train summary: loss 0.3853, accuracy 0.735\n",
      "Validation accuracy 0.779\n",
      "\n",
      "epoch  1, step  2250. Train summary: loss 0.3031, accuracy 0.815\n",
      "epoch  1, step  2500. Train summary: loss 0.2598, accuracy 0.805\n",
      "epoch  1, step  2750. Train summary: loss 0.3150, accuracy 0.775\n",
      "epoch  1, step  3000. Train summary: loss 0.4676, accuracy 0.642\n",
      "Validation accuracy 0.763\n",
      "\n",
      "epoch  1, step  3250. Train summary: loss 0.2678, accuracy 0.808\n",
      "epoch  1, step  3500. Train summary: loss 0.4133, accuracy 0.695\n",
      "epoch  1, step  3750. Train summary: loss 0.3921, accuracy 0.720\n",
      "epoch  1, step  4000. Train summary: loss 0.3279, accuracy 0.777\n",
      "Validation accuracy 0.764\n",
      "\n",
      "epoch  1, step  4250. Train summary: loss 0.3296, accuracy 0.752\n",
      "epoch  1, step  4500. Train summary: loss 0.4611, accuracy 0.640\n",
      "epoch  1, step  4750. Train summary: loss 0.4289, accuracy 0.685\n",
      "epoch  1, step  5000. Train summary: loss 0.3084, accuracy 0.788\n",
      "Validation accuracy 0.763\n",
      "\n",
      "epoch  1, step  5250. Train summary: loss 0.2138, accuracy 0.845\n",
      "epoch  1, step  5500. Train summary: loss 0.4189, accuracy 0.695\n",
      "epoch  1, step  5750. Train summary: loss 0.2799, accuracy 0.817\n",
      "epoch  1, step  6000. Train summary: loss 0.3090, accuracy 0.798\n",
      "Validation accuracy 0.778\n",
      "\n",
      "epoch  1, step  6250. Train summary: loss 0.4961, accuracy 0.623\n",
      "epoch  1, step  6500. Train summary: loss 0.2210, accuracy 0.848\n",
      "epoch  1, step  6750. Train summary: loss 0.4355, accuracy 0.655\n",
      "epoch  1, step  7000. Train summary: loss 0.2697, accuracy 0.803\n",
      "Validation accuracy 0.764\n",
      "\n",
      "epoch  1, step  7250. Train summary: loss 0.3822, accuracy 0.728\n",
      "epoch  1, step  7500. Train summary: loss 0.3889, accuracy 0.720\n",
      "epoch  1, step  7750. Train summary: loss 0.2550, accuracy 0.822\n",
      "epoch  1, step  8000. Train summary: loss 0.4159, accuracy 0.710\n",
      "Validation accuracy 0.763\n",
      "\n",
      "epoch  1, step  8250. Train summary: loss 0.3983, accuracy 0.680\n",
      "epoch  1, step  8500. Train summary: loss 0.3943, accuracy 0.710\n",
      "epoch  1, step  8750. Train summary: loss 0.3645, accuracy 0.770\n",
      "epoch  1, step  9000. Train summary: loss 0.4162, accuracy 0.695\n",
      "Validation accuracy 0.764\n",
      "\n",
      "epoch  1, step  9250. Train summary: loss 0.2586, accuracy 0.815\n",
      "epoch  1, step  9500. Train summary: loss 0.3274, accuracy 0.785\n",
      "epoch  1, step  9750. Train summary: loss 0.3859, accuracy 0.738\n",
      "epoch  1, step 10000. Train summary: loss 0.4048, accuracy 0.692\n",
      "Validation accuracy 0.760\n",
      "\n",
      "epoch  1, step 10250. Train summary: loss 0.2938, accuracy 0.752\n",
      "epoch  1, step 10500. Train summary: loss 0.3122, accuracy 0.757\n",
      "epoch  1, step 10750. Train summary: loss 0.3613, accuracy 0.733\n",
      "epoch  1, step 11000. Train summary: loss 0.4166, accuracy 0.710\n",
      "Validation accuracy 0.765\n",
      "\n",
      "epoch  1, step 11250. Train summary: loss 0.4250, accuracy 0.688\n",
      "epoch  1, step 11500. Train summary: loss 0.3942, accuracy 0.750\n",
      "epoch  1, step 11750. Train summary: loss 0.4224, accuracy 0.710\n",
      "epoch  1, step 12000. Train summary: loss 0.3177, accuracy 0.765\n",
      "Validation accuracy 0.764\n",
      "\n",
      "epoch  1, step 12250. Train summary: loss 0.3260, accuracy 0.775\n",
      "epoch  1, step 12500. Train summary: loss 0.4364, accuracy 0.705\n",
      "epoch  2, step 12750. Train summary: loss 0.3276, accuracy 0.752\n",
      "epoch  2, step 13000. Train summary: loss 0.2764, accuracy 0.788\n",
      "Validation accuracy 0.764\n",
      "\n",
      "epoch  2, step 13250. Train summary: loss 0.4501, accuracy 0.692\n",
      "epoch  2, step 13500. Train summary: loss 0.2892, accuracy 0.800\n",
      "epoch  2, step 13750. Train summary: loss 0.3905, accuracy 0.710\n",
      "epoch  2, step 14000. Train summary: loss 0.3876, accuracy 0.692\n",
      "Validation accuracy 0.765\n",
      "\n",
      "epoch  2, step 14250. Train summary: loss 0.4161, accuracy 0.700\n",
      "epoch  2, step 14500. Train summary: loss 0.4074, accuracy 0.707\n",
      "epoch  2, step 14750. Train summary: loss 0.3419, accuracy 0.743\n",
      "epoch  2, step 15000. Train summary: loss 0.2926, accuracy 0.775\n",
      "Validation accuracy 0.761\n",
      "\n",
      "epoch  2, step 15250. Train summary: loss 0.4147, accuracy 0.738\n",
      "epoch  2, step 15500. Train summary: loss 0.4092, accuracy 0.705\n",
      "epoch  2, step 15750. Train summary: loss 0.3426, accuracy 0.750\n",
      "epoch  2, step 16000. Train summary: loss 0.3866, accuracy 0.720\n",
      "Validation accuracy 0.761\n",
      "\n",
      "epoch  2, step 16250. Train summary: loss 0.3970, accuracy 0.710\n",
      "epoch  2, step 16500. Train summary: loss 0.2455, accuracy 0.815\n",
      "epoch  2, step 16750. Train summary: loss 0.3474, accuracy 0.780\n",
      "epoch  2, step 17000. Train summary: loss 0.3272, accuracy 0.765\n",
      "Validation accuracy 0.761\n",
      "\n",
      "epoch  2, step 17250. Train summary: loss 0.3327, accuracy 0.788\n",
      "epoch  2, step 17500. Train summary: loss 0.4142, accuracy 0.707\n",
      "epoch  2, step 17750. Train summary: loss 0.4959, accuracy 0.613\n",
      "epoch  2, step 18000. Train summary: loss 0.4331, accuracy 0.685\n",
      "Validation accuracy 0.757\n",
      "\n",
      "epoch  2, step 18250. Train summary: loss 0.3091, accuracy 0.760\n",
      "epoch  2, step 18500. Train summary: loss 0.3933, accuracy 0.717\n",
      "epoch  2, step 18750. Train summary: loss 0.3700, accuracy 0.720\n",
      "epoch  2, step 19000. Train summary: loss 0.2473, accuracy 0.817\n",
      "Validation accuracy 0.765\n",
      "\n",
      "epoch  2, step 19250. Train summary: loss 0.4550, accuracy 0.652\n",
      "epoch  2, step 19500. Train summary: loss 0.3617, accuracy 0.760\n",
      "epoch  2, step 19750. Train summary: loss 0.3091, accuracy 0.767\n",
      "epoch  2, step 20000. Train summary: loss 0.3316, accuracy 0.765\n",
      "Validation accuracy 0.759\n",
      "\n",
      "epoch  2, step 20250. Train summary: loss 0.4336, accuracy 0.725\n",
      "epoch  2, step 20500. Train summary: loss 0.3018, accuracy 0.775\n",
      "epoch  2, step 20750. Train summary: loss 0.4081, accuracy 0.705\n",
      "epoch  2, step 21000. Train summary: loss 0.3374, accuracy 0.745\n",
      "Validation accuracy 0.760\n",
      "\n",
      "epoch  2, step 21250. Train summary: loss 0.3729, accuracy 0.738\n",
      "epoch  2, step 21500. Train summary: loss 0.5251, accuracy 0.620\n",
      "epoch  2, step 21750. Train summary: loss 0.5236, accuracy 0.627\n",
      "epoch  2, step 22000. Train summary: loss 0.3686, accuracy 0.752\n",
      "Validation accuracy 0.765\n",
      "\n",
      "epoch  2, step 22250. Train summary: loss 0.2874, accuracy 0.812\n",
      "epoch  2, step 22500. Train summary: loss 0.3763, accuracy 0.723\n",
      "epoch  2, step 22750. Train summary: loss 0.2809, accuracy 0.805\n",
      "epoch  2, step 23000. Train summary: loss 0.4298, accuracy 0.692\n",
      "Validation accuracy 0.763\n",
      "\n",
      "epoch  2, step 23250. Train summary: loss 0.4724, accuracy 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-3e3b5a1e371b>\", line 14, in <module>\n",
      "    logits, predictions = model(inputs, lengths)\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-11-3624fd85f22b>\", line 17, in forward\n",
      "    inputs, lengths, batch_first=True).to(device)\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/torch/nn/utils/rnn.py\", line 99, in to\n",
      "    data = self.data.to(*args, **kwargs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/appl/python/3.6.2/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/appl/python/3.6.2/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/appl/python/3.6.2/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/appl/python/3.6.2/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/posixpath.py\", line 386, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/posixpath.py\", line 420, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/zhome/9d/d/98006/Documents/02456-Deep-learning/lib/python3.6/posixpath.py\", line 169, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "epochs = 5\n",
    "vary_lengths = True\n",
    "\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for inputs, targets in train_loader:\n",
    "        lengths = adjust_lengths(vary_lengths, inputs, targets)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, predictions = model(inputs, lengths)\n",
    "\n",
    "        # BCEWithLogitsLoss will do the activation\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        accuracy = ((predictions > 0.5) == (targets > 0.5)).type(torch.FloatTensor).mean()\n",
    "        train_accs.append((step, accuracy))\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            #Note: If you receive an error at the following line, you are not using a Python 3.6.x kernel\n",
    "            print(f'epoch {epoch:2d}, step {step:5d}. Train summary: loss {loss.item():.{4}f}, accuracy {accuracy:.{3}f}')\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            valid_accuracy = evaluate(max_bits, vary_lengths, device, model)\n",
    "            valid_accs.append((step, valid_accuracy))\n",
    "            print(f'Validation accuracy {valid_accuracy:.{3}f}\\n')\n",
    "            if valid_accuracy == 1.0:\n",
    "                # stop early\n",
    "                print('Stop early: valid_accuracy == 1.0')\n",
    "                break\n",
    "    if valid_accuracy == 1.0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(train_accs)[:,0], np.array(train_accs)[:,1], label='Training accuracy')\n",
    "plt.plot(np.array(valid_accs)[:,0], np.array(valid_accs)[:,1], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "1. The code provided uses fixed length sequences. Save your results, change `vary_lengths` to `True`, and run it again. Does the RNN still succeed? What explains the difference?\n",
    "1. Try and change the model architecture - e.g. number of units or number of layers. How does that affect performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Assignments\n",
    "1. Compare RNN and LSTM. Replace `torch.nn.RNN` with `torch.nn.LSTM` in order to use an LSTM instead. Compare the results of the LSTM and the RNN. Can you see a difference?\n",
    "1. Try and **remove teacher forcing**. This will require you to change the data generator `get_random_bits_parity` to use `np.sum` rather than `np.cumsum`. You also need to change some things in the training loop - use the error mesages to guide you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
